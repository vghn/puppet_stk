version: '3.7'

services:
  lb:
    image: traefik:1.7
    hostname: lb-prometheus
    command: >-
      --configfile=/dev/null
      --web --web.address=0.0.0.0:8888
      --docker --docker.swarmmode --docker.domain=traefik --docker.watch
      --defaultentrypoints=http,https
      --entryPoints="Name:http Address::80 Redirect.EntryPoint:https"
      --entryPoints="Name:https Address::443 TLS"
      --acme --acme.email=webmaster@ghn.me
      --acme.domains=prometheus.ghn.me
      --acme.tlsChallenge
      --acme.entryPoint=https
      --acme.storage=/certs/acme.json
    ports:
      - '80:80'
      - '443:443'
      - '8888:8888'
    volumes:
      - /data/certs:/certs
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - frontend
      - backend
    labels:
      container_group: monitoring
    deploy:
      placement:
        constraints:
          - node.role == manager
          - node.labels.role == prometheus
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.05'
          memory: 64M
  grafana:
    image: grafana/grafana:6.4.1
    hostname: grafana-prometheus
    environment:
      GF_SERVER_ROOT_URL: https://prometheus.ghn.me
      GF_SMTP_ENABLED: 'true'
      GF_SMTP_HOST: smtp.sendgrid.net:587
      GF_SMTP_FROM_ADDRESS: admin@ghn.me
      GF_SMTP_FROM_NAME: Prometheus
      GF_SECURITY_ADMIN_PASSWORD_FILE: /run/secrets/grafana_security_admin_password
      GF_SMTP_USER_FILE: /run/secrets/grafana_smtp_user
      GF_SMTP_PASSWORD_FILE: /run/secrets/grafana_smtp_password
    secrets:
      - grafana_security_admin_password
      - grafana_smtp_user
      - grafana_smtp_password
    volumes:
      - /data/grafana:/var/lib/grafana
    networks:
      - backend
    labels:
      container_group: monitoring
    deploy:
      placement:
        constraints:
          - node.labels.role == prometheus
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.10'
          memory: 64M
      labels:
        traefik.tags: 'grafana'
        traefik.port: '3000'
        traefik.frontend.rule: 'Host:prometheus.ghn.me;PathPrefix:/'
        traefik.backend.loadbalancer.sticky: 'true'
        traefik.frontend.headers.browserXSSFilter: 'true'
        traefik.frontend.headers.contentTypeNosniff: 'true'
        traefik.frontend.headers.referrerPolicy: 'no-referrer'
        traefik.frontend.headers.frameDeny: 'true'
        traefik.frontend.headers.contentSecurityPolicy: "default-src https: blob: data:; script-src 'self' 'unsafe-inline' 'unsafe-eval'; style-src 'self' 'unsafe-inline' 'unsafe-eval'"
        traefik.frontend.headers.SSLRedirect: 'true'
        traefik.frontend.headers.STSSeconds: '31536000'
        traefik.frontend.headers.forceSTSHeader: 'true'
        traefik.frontend.headers.SSLHost: 'prometheus.ghn.me'
        traefik.frontend.headers.STSIncludeSubdomains: 'true'
        traefik.frontend.headers.STSPreload: 'true'
  prometheus:
    image: prom/prometheus:v2.12.0
    hostname: prometheus-prometheus
    configs:
      - source: alertrules.yml
        target: /etc/prometheus/alertrules.yml
      - source: prometheus.yml
        target: /etc/prometheus/prometheus.yml
    volumes:
      - /data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
    ports:
      - '9090:9090'
    networks:
      - backend
    labels:
      container_group: monitoring
    deploy:
      placement:
        constraints:
          - node.labels.role == prometheus
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.30'
          memory: 512M
  alertmanager:
    image: prom/alertmanager:v0.19.0
    hostname: alertmanager-prometheus
    secrets:
      - alertmanager_config.yml
    volumes:
      - /data/alertmanager:/alertmanager
    command:
      - '--storage.path=/alertmanager'
      - '--config.file=/run/secrets/alertmanager_config.yml'
    ports:
      - '9093:9093'
    networks:
      - backend
    labels:
      container_group: monitoring
    deploy:
      placement:
        constraints:
          - node.labels.role == prometheus
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.05'
          memory: 32M
  rsyslog:
    image: vladgh/rsyslog:latest
    hostname: logs.ghn.me
    environment:
      LOGZIO_TOKEN_FILE: '/run/secrets/logs_logzio_token'
      CA_CERT: '/run/secrets/logs_combined_ca_cert.pem'
      SERVER_CERT: '/run/secrets/logs_server_cert.pem'
      SERVER_KEY: '/run/secrets/logs_server_key.pem'
      TIME_ZONE: 'US/Central'
      REMOTE_LOGS_PATH: '/data/logs'
    secrets:
      - logs_combined_ca_cert.pem
      - logs_server_cert.pem
      - logs_server_key.pem
      - logs_logzio_token
    volumes:
      - /data/logs:/data/logs
    ports:
      - '10514:10514'
    networks:
      - frontend
    labels:
      container_group: logs
    deploy:
      placement:
        constraints:
          - node.labels.role == prometheus
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.10'
          memory: 64M
  log:
    image: gliderlabs/logspout:v3.2.6
    hostname: logs-prometheus
    configs:
      - source: VladGhCARoot.crt
        target: /usr/local/share/ca-certificates/VladGhCARoot.crt
    environment:
      BACKLOG: 'false'
      EXCLUDE_LABEL: 'logspout.exclude'
      LOGSPOUT_TLS_CA_CERTS: '/usr/local/share/ca-certificates/VladGhCARoot.crt'
    command: syslog+tls://logs.ghn.me:10514
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - backend
    labels:
      container_group: logs
    deploy:
      mode: global
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.05'
          memory: 32M
  logrotate:
    image: vladgh/logrotate:latest
    hostname: logrotate-prometheus
    volumes:
      - /data/logs:/logs
    networks:
      - backend
    labels:
      container_group: logs
    deploy:
      mode: global
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.05'
          memory: 32M
  cadvisor:
    image: google/cadvisor:v0.33.0
    hostname: cadvisor-prometheus
    volumes:
      - /:/rootfs:ro
      - /dev/disk/:/dev/disk:ro
      - /sys:/sys:ro
      - /var/run:/var/run:ro
      - /var/lib/docker/:/var/lib/docker:ro
    command:
      - '--housekeeping_interval=1m'
      - '--global_housekeeping_interval=2m'
    ports:
      - '9101:8080'
    networks:
      - backend
    labels:
      container_group: monitoring
    deploy:
      mode: global
      restart_policy:
        delay: 10s
        max_attempts: 10
        window: 60s
      resources:
        limits:
          cpus: '0.20'
          memory: 64M

secrets:
  alertmanager_config.yml:
    name: alertmanager_config.yml_v1
    external: true
  grafana_security_admin_password:
    name: grafana_security_admin_password_v1
    external: true
  grafana_smtp_password:
    name: grafana_smtp_password_v1
    external: true
  grafana_smtp_user:
    name: grafana_smtp_user_v1
    external: true
  logs_combined_ca_cert.pem:
    name: logs_combined_ca_cert.pem_v1
    external: true
  logs_server_cert.pem:
    name: logs_server_cert.pem_v1
    external: true
  logs_server_key.pem:
    name: logs_server_key.pem_v1
    external: true
  logs_logzio_token:
    name: logs_logzio_token_v1
    external: true

configs:
  alertrules.yml:
    name: alertrules.yml_v6
    external: true
  prometheus.yml:
    name: prometheus.yml_v17
    external: true
  VladGhCARoot.crt:
    name: VladGhCARoot.crt_v1
    external: true

networks:
  frontend:
    ipam:
      config:
        - subnet: 10.1.0.0/24 # Should not conflict with the AWS VPC
  backend:
    ipam:
      config:
        - subnet: 10.2.0.0/24 # Should not conflict with the AWS VPC
